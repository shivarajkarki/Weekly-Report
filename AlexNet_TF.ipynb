{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n",
      "WARNING:tensorflow:From C:\\Users\\aim\\Anaconda3\\envs\\tensorflowsessions\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn.datasets.oxflower17 as oxflower17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Oxford 17 category Flower Dataset, Please wait...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0% 60276736 / 60270631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Succesfully downloaded', '17flowers.tgz', 60270631, 'bytes.')\n",
      "File Extracted\n",
      "Starting to parse images...\n",
      "Parsing Done!\n"
     ]
    }
   ],
   "source": [
    "X, Y = oxflower17.load_data(one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x: (1360, 224, 224, 3)\n",
      "shape of y: (1360, 17)\n",
      "Sample y: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print('Shape of x: {}'.format(X.shape))\n",
    "print('shape of y: {}'.format(Y.shape))\n",
    "print(\"Sample y: {}\".format(Y[0]))\n",
    "\n",
    "\n",
    "categories = ['Buttercup', 'Colts Foot', 'Daffodil', 'Daisy', 'Dandelion', 'Fritillary', 'Iris', 'Pansy', \n",
    "              'Sunflower', 'WindFlower', 'SnowDrop', 'LilyValley', 'Bluebell', 'Crocus', 'Tigerlilly', 'Tulip',\n",
    "              'CowSlip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sahape of X_train: (1360, 227, 227, 3)\n"
     ]
    }
   ],
   "source": [
    "paddings = ((0,0), (2,1), (2,1), (0,0))\n",
    "X_train = np.pad(X, paddings, 'constant')\n",
    "print(\"Sahape of X_train: {}\".format(X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size...\n",
      "x_train: (1101, 227, 227, 3)\n",
      "y_train: (1101, 17)\n",
      "\n",
      "Validation set size...\n",
      "x_val: (123, 227, 227, 3)\n",
      "y_val: (123, 17)\n",
      "\n",
      "Test set size...\n",
      "x_test: (136, 227, 227, 3)\n",
      "y_test: (136, 17)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_train, Y, test_size=0.1, random_state=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)\n",
    "    \n",
    "print(\"Train set size...\\nx_train: {}\\ny_train: {}\\n\".format(x_train.shape, y_train.shape))\n",
    "\n",
    "print(\"Validation set size...\\nx_val: {}\\ny_val: {}\\n\".format(x_val.shape, y_val.shape))\n",
    "\n",
    "print(\"Test set size...\\nx_test: {}\\ny_test: {}\\n\".format(x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data is ready... \n",
    "def AlexNet(x):\n",
    "    \n",
    "    #.. input shape (227,227,3)\n",
    "    \"\"\" filter shape (11,11), stride 4, no.of.filters = 96, activation is relu\"\"\"\n",
    "    conv1_W = tf.Variable(tf.truncated_normal(mean = 0, stddev =1.0, shape = [11,11,3,96]))\n",
    "    conv1_b = tf.Variable(tf.zeros(96))\n",
    "    conv1 = tf.nn.conv2d(x, conv1_W, strides=[1,4,4,1], padding='VALID') + conv1_b\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    \n",
    "    \"\"\"max pooling... filter size (3,3) and strides = 2\"\"\"\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID')\n",
    "    ##....layer 1 is done....##\n",
    "    \n",
    "    #****\n",
    "    #local response normalization\n",
    "    #https://medium.com/@pechyonkin/key-deep-learning-architectures-alexnet-30bf607595f1\n",
    "    #https://www.tensorflow.org/api_docs/python/tf/nn/local_response_normalization\n",
    "    lrn1 = tf.nn.local_response_normalization(pool1, alpha=10**-4, beta=0.75, bias=2)\n",
    "    #***\n",
    "    \n",
    "    #.. input size (27,27,96)\n",
    "    \"\"\"filter shape(5,5), padding = same, stride 1, filters= 256 \"\"\"\n",
    "    conv2_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = [5,5,96,256]))\n",
    "    conv2_b = tf.Variable(tf.zeros(256))\n",
    "    conv2 = tf.nn.conv2d(lrn1, conv2_W, strides=[1,1,1,1], padding='SAME') + conv2_b\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    \n",
    "    \"\"\"max pooling... filter_size (3,3) and stride = 2 \"\"\"\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID')\n",
    "    ##....layer 2 is done....##\n",
    "    \n",
    "    #***\n",
    "    #Local Response Normalization\n",
    "    lrn2 = tf.nn.local_response_normalization(pool2, alpha=10**-4, beta=0.75, bias=1)\n",
    "    #***\n",
    "    \n",
    "    #.. input size (13,13,256)\n",
    "    \"\"\"filter shape(3,3), padding = same, stride = 1, filters = 384\"\"\"\n",
    "    conv3_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = [3,3,256,384]))\n",
    "    conv3_b = tf.Variable(tf.zeros(384))\n",
    "    conv3 = tf.nn.conv2d(lrn2, conv3_W, strides=[1,1,1,1], padding='SAME') + conv3_b\n",
    "    conv3 = tf.nn.relu(conv3)\n",
    "    ##....layer 3 is done....##\n",
    "    \n",
    "    #.. input size (13,13,384)\n",
    "    \"\"\"filter shape(3,3), padding = same, stride = 1, filters = 384\"\"\"\n",
    "    conv4_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = [3,3,384,384]))\n",
    "    conv4_b = tf.Variable(tf.zeros(384))\n",
    "    conv4 = tf.nn.conv2d(conv3, conv4_W, strides=[1,1,1,1], padding='SAME') + conv4_b\n",
    "    conv4 = tf.nn.relu(conv4)\n",
    "    ##....layer 4 is done....##\n",
    "    \n",
    "    #.. input size (13,13,384)\n",
    "    \"\"\"filter shape(3,3), padding = same, stride = 1, filters = 256\"\"\"\n",
    "    conv5_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = [3,3,384,256]))\n",
    "    conv5_b = tf.Variable(tf.zeros(256))\n",
    "    conv5 = tf.nn.conv2d(conv4, conv5_W, strides=[1,1,1,1], padding='SAME') + conv5_b\n",
    "    conv5 = tf.nn.relu(conv5)\n",
    "    \n",
    "    #.. input size (13,13,256)\n",
    "    \"\"\"max pooling filter size = (3,3), stride = 2\"\"\"\n",
    "    pool5 = tf.nn.max_pool(conv5, ksize=[1,3,3,1], strides=[1,2,2,1], padding='VALID')\n",
    "    ##....layer 5 is done....##\n",
    "    \n",
    "    #.. input size (6,6,256) \n",
    "    \"\"\"Fully connected layer with drop out(probability = 0.5)\"\"\"\n",
    "    flat = tf.layers.flatten(pool5)\n",
    "    \n",
    "    #.. input size = 6*6*256=9216\n",
    "    fc6_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = (9216,4096)))\n",
    "    fc6_b = tf.Variable(tf.zeros(4096))\n",
    "    \n",
    "    dropped1 = tf.nn.dropout(flat, keep_prob=0.5) #dropout\n",
    "    fc6 = tf.matmul(dropped1, fc6_W) + fc6_b\n",
    "    \n",
    "    fc6 = tf.nn.relu(fc6)\n",
    "    \n",
    "    #.. input size 4096\n",
    "    fc7_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = (4096,4096)))\n",
    "    fc7_b = tf.Variable(tf.zeros(4096))\n",
    "    \n",
    "    dropped2 = tf.nn.dropout(fc6, keep_prob=0.5) #dropout\n",
    "    fc7 = tf.matmul(dropped2, fc7_W) + fc7_b\n",
    "    \n",
    "    fc7 = tf.nn.relu(fc7)\n",
    "    \n",
    "    #.. input size 4096\n",
    "    fc8_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = (4096, 1000)))\n",
    "    fc8_b = tf.Variable(tf.zeros(1000))\n",
    "    \n",
    "    fc8 = tf.matmul(fc7, fc8_W) + fc8_b\n",
    "    \n",
    "    fc8 = tf.nn.relu(fc8)\n",
    "    \n",
    "    #.. input size 1000... output needs to be classified out of 17 classes\n",
    "    fc9_W = tf.Variable(tf.truncated_normal(mean = 0, stddev = 1.0, shape = (1000,17)))\n",
    "    fc9_b = tf.Variable(tf.zeros(17))\n",
    "    \n",
    "    fc9 = tf.matmul(fc8, fc9_W) + fc9_b \n",
    "    \n",
    "    return fc9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, (None, 227, 227, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = AlexNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "training_operation = optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  num_examples = len(x_train)\n",
    "  \n",
    "  print(\"Training....\\n\")\n",
    "  print(\"For Batch size...\", BATCH_SIZE)\n",
    "  \n",
    "  for i in range(EPOCHS):\n",
    "    start = time.process_time()\n",
    "    \n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "      end = offset + BATCH_SIZE\n",
    "\n",
    "      batch_x, batch_y = x_train[offset:end], y_train[offset:end]\n",
    "      sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "      \n",
    "    \n",
    "\n",
    "    validation_accuracy = evaluate(validation_x, validation_label)\n",
    "    print(\"EPOCH {} ...\".format(i+1))\n",
    "    print(\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "    print(\"Time taken by Epoch: \",i ,time.process_time() - start)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
