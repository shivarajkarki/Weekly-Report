{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,6):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 1\n",
      "43 22\n",
      "64 43\n",
      "90 64\n"
     ]
    }
   ],
   "source": [
    "l =[1, 22, 43,64, 90]\n",
    "for i in range(1, len(l)):\n",
    "    print(l[i], l[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Initializing_parameters(layer_dims):     #(no.of.input, #unit.in.layer.1, ...., #unit.in.layer.L-1)\n",
    "    \n",
    "    parameters = {}  #create Dictionary\n",
    "    \n",
    "    for i in range(1,len(layer_dims)):\n",
    "        \"\"\"create W and b with (l, l-1) and (l, 1) dim respectively\"\"\"\n",
    "        #print(layer_dims[i], layer_dims[i-1]) #debug\n",
    "        parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "        parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "     \n",
    "    #***\n",
    "    #debug\n",
    "    #for key in parameter:\n",
    "        #print(key)\n",
    "    #***\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Forward Propogation need following\n",
    "\n",
    "$Z^{[l]}$ ,$A^{[l-1]}$, $W^{[l]}$, $b^{[l]}$, $g^{[l]}(Z^{[l]})$\n",
    "\n",
    "\n",
    "if $g^{[l]}(Z^{[l]})$ is relu then\n",
    "\\begin{align}\n",
    "g^{[l]}(Z^{[l]}) = max(0,Z^{[l]})\n",
    "\\end{align}\n",
    "\n",
    "if $g^{[l]}(Z^{[l]})$ is sigmoid then\n",
    "\\begin{align}\n",
    "g^{[l]}(Z^{[l]}) = \\frac{1}{1+e^{-Z^{[l]}}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Forwar_propogation(X, parameters, Layers):\n",
    "    \n",
    "    #no_of_features = X.shape[0]\n",
    "    #Layers = [no_of_features,7,5,3,1] #define no_of_units_in_each_layer\n",
    "    \n",
    "    L =len(Layers)\n",
    "    \n",
    "    #parameters = Initializing_parameters(Layers)\n",
    "    \n",
    "    A_prev = X\n",
    "    \n",
    "    for i in range(1, L-1):\n",
    "        #print('Layer', i) #debug\n",
    "        \"\"\"Update dictionary parameters with Z = WA + b\"\"\"\n",
    "        \"\"\"Activation function for layers from 1 to L-1 is relu\"\"\"\n",
    "        \n",
    "        parameters['Z' + str(i)] = np.dot(parameters['W' + str(i)], A_prev) + parameters['b' + str(i)]\n",
    "        O = np.zeros(parameters['Z' + str(i)].shape)\n",
    "        #print(O) #debug\n",
    "        parameters['A' + str(i)] = np.maximum(O, parameters['Z' + str(i)])  #relu = max(0, Z)\n",
    "        A_prev = parameters['A' + str(i)]\n",
    "        \n",
    "        #***debug\n",
    "        #print(parameters['Z' + str(i)].shape,'=' ,parameters['W' + str(i)].shape,A_prev.shape)\n",
    "        #A_prev = parameters['Z' + str(i)] #debug\n",
    "        #parameters['A' + str(i)] = parameters['Z' + str(i)] #debug\n",
    "        #***\n",
    "        \n",
    "    #***debug\n",
    "    #print('Layer', L-1)\n",
    "    #print('=' ,parameters['W' + str(L-1)].shape)\n",
    "    \n",
    "    #***\n",
    "    \n",
    "    parameters['Z' + str(L-1)] = np.dot(parameters['W' + str(L-1)], parameters['A' + str(L-2)]) + parameters['b' + str(L-1)]\n",
    "    parameters['A' + str(L-1)] = 1 / (1 + np.exp(-parameters['Z' + str(L-1)]))  #sigmoid for last layer\n",
    "    \n",
    "    #***debug\n",
    "    #for key in parameters:\n",
    "    #    print(key)\n",
    "    #***\n",
    "\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (7, 3)\n",
      "b1 (7, 1)\n",
      "W2 (5, 7)\n",
      "b2 (5, 1)\n",
      "W3 (3, 5)\n",
      "b3 (3, 1)\n",
      "W4 (1, 3)\n",
      "b4 (1, 1)\n",
      "Z1 (7, 2)\n",
      "A1 (7, 2)\n",
      "Z2 (5, 2)\n",
      "A2 (5, 2)\n",
      "Z3 (3, 2)\n",
      "A3 (3, 2)\n",
      "Z4 (1, 2)\n",
      "A4 (1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test Purpose\n",
    "\"\"\n",
    "X = np.array([[1, 2],\n",
    "             [3, 4],\n",
    "             [5, 6]])\n",
    "\n",
    "no_of_features = X.shape[0]\n",
    "Layers = [no_of_features,7,5,3,1]\n",
    "parameters = Initializing_parameters(Layers)\n",
    "\n",
    "\n",
    "parameters = Forwar_propogation(X, parameters,Layers)\n",
    "\n",
    "for key in parameters:\n",
    "    print(key,parameters[str(key)].shape)\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Calculate_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(1/m) * (np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for l in reversed(range(1, 6)):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 2]] \n",
      " [[0]\n",
      " [1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "ex_dic = {'a': np.arange(0,3).reshape(1,3),\n",
    "         'b': [[5,6,7]]}\n",
    "a = ex_dic['a']\n",
    "print(a,'\\n',a.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Backward_popogation(X,Y, parameters):\n",
    "    \"\"\"need to update W, L based on learning rate using gradient decent\"\"\"\n",
    "    \n",
    "    \"\"\"last layer has sigmoid activation function, and rest have relu\"\"\"\n",
    "    \n",
    "    \"\"\"dZL, dZ, dW, db, dA\"\"\"\n",
    "    \n",
    "    grads = {} #creating dictionary for gradient values\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    L = 4\n",
    "    \n",
    "    parameters['A0'] = X\n",
    "    \n",
    "    #--- for last layer (L)\n",
    "    AL = parameters['A' + str(L)]\n",
    "    grads['dZ' + str(L)] = AL - Y\n",
    "    \n",
    "    dZL = grads['dZ' + str(L)]\n",
    "    AL_1 = parameters['A' + str(L-1)]\n",
    "    grads['dW' + str(L)] = (1/m) * np.dot(dZL, AL_1.T)\n",
    "    \n",
    "    grads['db' + str(L)] = (1/m) * np.sum(dZL, axis = 1, keepdims=True)\n",
    "    \n",
    "    W = parameters['W' + str(L)]\n",
    "    grads['dA' + str(L-1)] = np.dot(W.T, dZL)\n",
    "    #-----****\n",
    "    \n",
    "    #--- for layers 1 to L-1\n",
    "    for l in reversed(range(1, L)):\n",
    "        \n",
    "        W1 =  parameters['W' + str(l+1)]\n",
    "        dZ1 = grads['dZ' + str(l+1)]\n",
    "        grads['dZ' + str(l)] = np.dot(W1.T, dZ1)\n",
    "        \n",
    "        \n",
    "        dZ = grads['dZ' + str(l)]\n",
    "        A_1 = parameters['A' + str(l-1)]\n",
    "        grads['dW' + str(l)] = (1/m) * np.dot(dZ, A_1.T)\n",
    "        \n",
    "        grads['db' + str(l)] = (1/m) * np.sum(dZ, axis = 1, keepdims=True)\n",
    "        \n",
    "        W = parameters['W'+str(l)]\n",
    "        grads['dA' + str(l-1)] = np.dot(W.T, dZ)\n",
    "        \n",
    "    #-----****\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, Learning_rate):\n",
    "    \n",
    "    L = 4\n",
    "    \n",
    "    for i in range(1, L+1):\n",
    "        parameters['W' + str(i)] = parameters['W' + str(i)] - Learning_rate * grads['dW' + str(i)]\n",
    "        parameters['b' + str(i)] = parameters['b' + str(i)] - Learning_rate * grads['db' + str(i)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_Network_Model(X, Y, No_of_iterations, Learning_rate):\n",
    "    \n",
    "    no_of_features = X.shape[0]\n",
    "    Layers = [no_of_features,1000,1000,200,1] #define no_of_units_in_each_layer\n",
    " \n",
    "    set_parameters = Initializing_parameters(Layers)\n",
    "    \n",
    "    L = 4\n",
    "    \n",
    "    for i in range(0, No_of_iterations):\n",
    "        \n",
    "        ### calculate W, B, Z, and A for all L layers\n",
    "        parameters = Forwar_propogation(X, set_parameters, Layers)\n",
    "        \n",
    "        #.... Calculte Cost\n",
    "        AL = parameters['A' + str(L)]\n",
    "        cost = Calculate_cost(AL, Y)\n",
    "        \n",
    "        #.... Backward Propogation\n",
    "        grads = Backward_popogation(X, Y, parameters)\n",
    "        \n",
    "        set_parameters = update_parameters(parameters, grads, Learning_rate)\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print(\"for\",i,\"th iteration\",cost)\n",
    "            #print(AL.shape)\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters, Layers):\n",
    "    \n",
    "    m = len(Layers)-1\n",
    "    param = Forwar_propogation(X, parameters, Layers)\n",
    "    \n",
    "    Y_pred = []\n",
    "    \n",
    "    Y_pred = param['A'+str(m)]\n",
    "    \n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###-----******YES....!!!!,,,, Its Time to Test....******###\n",
    "\n",
    "### Arrange the Data and Preprocess it,,,\n",
    "\n",
    "import h5py\n",
    "\n",
    "def load_dataset():\n",
    "    train_dataset = h5py.File('Downloads/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('Downloads/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n",
    "\n",
    "### retrive images\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_dataset()\n",
    "\n",
    "### Flatten them into matrix form\n",
    "train_set_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0],-1).T\n",
    "test_set_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0],-1).T\n",
    "\n",
    "### Normalizing the matrices\n",
    "train_set_x = train_set_flatten/255\n",
    "test_set_x = test_set_flatten/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trian set shape(No_of_features, No_of_examples) (12288, 209)\n",
      "test set shape(No_of_features, No_of_examples) (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "print('trian set shape(No_of_features, No_of_examples)',train_set_x.shape)\n",
    "print('test set shape(No_of_features, No_of_examples)', test_set_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0 th iteration [[0.69372398]]\n",
      "for 1000 th iteration [[0.64405779]]\n",
      "for 2000 th iteration [[0.63003872]]\n",
      "for 3000 th iteration [[0.57049065]]\n",
      "for 4000 th iteration [[0.34170866]]\n",
      "for 5000 th iteration [[0.01615204]]\n",
      "for 6000 th iteration [[0.00390624]]\n",
      "for 7000 th iteration [[0.001967]]\n",
      "for 8000 th iteration [[0.00125093]]\n",
      "for 9000 th iteration [[0.00089406]]\n"
     ]
    }
   ],
   "source": [
    "parameters = Neural_Network_Model(train_set_x, train_set_y_orig, 10000, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1\n",
      "b1\n",
      "W2\n",
      "b2\n",
      "W3\n",
      "b3\n",
      "W4\n",
      "b4\n",
      "Z1\n",
      "A1\n",
      "Z2\n",
      "A2\n",
      "Z3\n",
      "A3\n",
      "Z4\n",
      "A4\n",
      "A0\n"
     ]
    }
   ],
   "source": [
    "for key in parameters:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = predict(train_set_x, train_set_y_orig, parameters, Layers = [train_set_x.shape[0],7,5,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = Y_pred > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_pred == train_set_y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
