{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dataset = tfds.list_builders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abstract_reasoning',\n",
       " 'bair_robot_pushing_small',\n",
       " 'caltech101',\n",
       " 'cats_vs_dogs',\n",
       " 'celeb_a',\n",
       " 'celeb_a_hq',\n",
       " 'chexpert',\n",
       " 'cifar10',\n",
       " 'cifar100',\n",
       " 'cifar10_corrupted',\n",
       " 'cnn_dailymail',\n",
       " 'coco2014',\n",
       " 'colorectal_histology',\n",
       " 'colorectal_histology_large',\n",
       " 'cycle_gan',\n",
       " 'diabetic_retinopathy_detection',\n",
       " 'dsprites',\n",
       " 'dtd',\n",
       " 'dummy_dataset_shared_generator',\n",
       " 'dummy_mnist',\n",
       " 'emnist',\n",
       " 'fashion_mnist',\n",
       " 'flores',\n",
       " 'glue',\n",
       " 'groove',\n",
       " 'higgs',\n",
       " 'horses_or_humans',\n",
       " 'image_label_folder',\n",
       " 'imagenet2012',\n",
       " 'imagenet2012_corrupted',\n",
       " 'imdb_reviews',\n",
       " 'iris',\n",
       " 'kmnist',\n",
       " 'lm1b',\n",
       " 'lsun',\n",
       " 'mnist',\n",
       " 'moving_mnist',\n",
       " 'multi_nli',\n",
       " 'nsynth',\n",
       " 'omniglot',\n",
       " 'open_images_v4',\n",
       " 'oxford_flowers102',\n",
       " 'oxford_iiit_pet',\n",
       " 'para_crawl',\n",
       " 'quickdraw_bitmap',\n",
       " 'rock_paper_scissors',\n",
       " 'shapes3d',\n",
       " 'smallnorb',\n",
       " 'squad',\n",
       " 'starcraft_video',\n",
       " 'sun397',\n",
       " 'svhn_cropped',\n",
       " 'ted_hrlr_translate',\n",
       " 'ted_multi_translate',\n",
       " 'tf_flowers',\n",
       " 'titanic',\n",
       " 'ucf101',\n",
       " 'voc2007',\n",
       " 'wikipedia',\n",
       " 'wmt15_translate',\n",
       " 'wmt16_translate',\n",
       " 'wmt17_translate',\n",
       " 'wmt18_translate',\n",
       " 'wmt19_translate',\n",
       " 'wmt_translate',\n",
       " 'xnli']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test,y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train Data: (60000, 28, 28)\n",
      "type: <class 'numpy.ndarray'>\n",
      "\n",
      "Shape of X test Data: (10000, 28, 28)\n",
      "type: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X train Data: {}\\ntype: {}\\n\".format(x_train.shape, type(x_train)))\n",
    "print(\"Shape of X test Data: {}\\ntype: {}\".format(x_test.shape, type(x_test)))\n",
    "num_train_examples = x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xe5d1c18>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_index = 7868\n",
    "plt.xlabel(y_train[image_index])\n",
    "plt.imshow(x_train[image_index], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 32, 32, 1)\n",
      "Number of images in x_train 60000\n",
      "Number of images in x_test 10000\n"
     ]
    }
   ],
   "source": [
    "#converting shape to 4D\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "\n",
    "#Padding the images by 2 pixels since in the paper input images were 32x32\n",
    "x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "\n",
    "#converting to float data type\n",
    "#x_train = x_train.astype('float32')\n",
    "#x_test = x_test.astype('float32')\n",
    "\n",
    "#Normalizing the data\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('Number of images in x_train', x_train.shape[0])\n",
    "print('Number of images in x_test', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets code LeNet\n",
    "l0 = tf.keras.layers.Conv2D(filters=6, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(32,32,1))\n",
    "l1 = tf.keras.layers.AveragePooling2D(pool_size = 2, strides = 2)\n",
    "l2 = tf.keras.layers.Conv2D(filters=16, kernel_size=(3,3), strides=1, activation='relu')\n",
    "l3 = tf.keras.layers.AveragePooling2D(pool_size = 2, strides = 2)\n",
    "l_inter_med = tf.keras.layers.Flatten()\n",
    "l4 = tf.keras.layers.Dense(units=120, activation=tf.nn.relu)\n",
    "l5 = tf.keras.layers.Dense(units=84, activation=tf.nn.relu)\n",
    "l6 = tf.keras.layers.Dense(units=10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\aim\\Anaconda3\\envs\\tensorflowsessions\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0717 08:35:46.143743  8340 deprecation.py:323] From C:\\Users\\aim\\Anaconda3\\envs\\tensorflowsessions\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([l0, l1, l2, l3, l_inter_med, l4, l5, l6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 185/1875 [=>............................] - ETA: 29:04:36 - loss: 2.2959 - acc: 0.121 - ETA: 35:41:27 - loss: 2.2864 - acc: 0.203 - ETA: 40:41:54 - loss: 2.2771 - acc: 0.252 - ETA: 44:21:13 - loss: 2.2673 - acc: 0.283 - ETA: 46:27:50 - loss: 2.2571 - acc: 0.306 - ETA: 47:43:48 - loss: 2.2461 - acc: 0.324 - ETA: 47:00:32 - loss: 2.2341 - acc: 0.341 - ETA: 47:56:17 - loss: 2.2211 - acc: 0.358 - ETA: 49:20:52 - loss: 2.2068 - acc: 0.375 - ETA: 49:04:48 - loss: 2.1911 - acc: 0.391 - ETA: 50:39:43 - loss: 2.1739 - acc: 0.407 - ETA: 52:02:03 - loss: 2.1552 - acc: 0.423 - ETA: 51:26:36 - loss: 2.1350 - acc: 0.438 - ETA: 51:31:19 - loss: 2.1132 - acc: 0.453 - ETA: 51:17:49 - loss: 2.0900 - acc: 0.468 - ETA: 50:27:06 - loss: 2.0652 - acc: 0.482 - ETA: 50:17:46 - loss: 2.0390 - acc: 0.496 - ETA: 50:01:04 - loss: 2.0114 - acc: 0.509 - ETA: 49:19:46 - loss: 1.9825 - acc: 0.522 - ETA: 49:14:30 - loss: 1.9524 - acc: 0.533 - ETA: 48:46:21 - loss: 1.9213 - acc: 0.544 - ETA: 48:10:10 - loss: 1.8893 - acc: 0.554 - ETA: 48:08:56 - loss: 1.8567 - acc: 0.563 - ETA: 48:06:10 - loss: 1.8236 - acc: 0.572 - ETA: 47:42:15 - loss: 1.7903 - acc: 0.580 - ETA: 47:42:00 - loss: 1.7569 - acc: 0.588 - ETA: 47:33:40 - loss: 1.7237 - acc: 0.596 - ETA: 47:07:40 - loss: 1.6908 - acc: 0.603 - ETA: 47:05:24 - loss: 1.6584 - acc: 0.610 - ETA: 47:12:24 - loss: 1.6267 - acc: 0.617 - ETA: 46:53:56 - loss: 1.5957 - acc: 0.624 - ETA: 47:00:57 - loss: 1.5655 - acc: 0.630 - ETA: 47:04:09 - loss: 1.5362 - acc: 0.636 - ETA: 47:01:34 - loss: 1.5078 - acc: 0.642 - ETA: 46:43:21 - loss: 1.4804 - acc: 0.648 - ETA: 46:40:25 - loss: 1.4539 - acc: 0.653 - ETA: 46:38:29 - loss: 1.4283 - acc: 0.659 - ETA: 46:37:57 - loss: 1.4036 - acc: 0.664 - ETA: 46:46:31 - loss: 1.3797 - acc: 0.669 - ETA: 46:50:26 - loss: 1.3567 - acc: 0.674 - ETA: 46:48:18 - loss: 1.3346 - acc: 0.679 - ETA: 46:35:42 - loss: 1.3132 - acc: 0.683 - ETA: 46:30:59 - loss: 1.2927 - acc: 0.688 - ETA: 46:35:26 - loss: 1.2728 - acc: 0.692 - ETA: 46:21:14 - loss: 1.2537 - acc: 0.696 - ETA: 46:16:22 - loss: 1.2352 - acc: 0.700 - ETA: 45:59:14 - loss: 1.2173 - acc: 0.704 - ETA: 45:45:38 - loss: 1.2001 - acc: 0.708 - ETA: 45:44:19 - loss: 1.1834 - acc: 0.711 - ETA: 45:43:17 - loss: 1.1672 - acc: 0.715 - ETA: 45:44:30 - loss: 1.1516 - acc: 0.718 - ETA: 45:44:47 - loss: 1.1364 - acc: 0.722 - ETA: 45:48:51 - loss: 1.1217 - acc: 0.725 - ETA: 45:41:23 - loss: 1.1075 - acc: 0.728 - ETA: 45:35:02 - loss: 1.0937 - acc: 0.731 - ETA: 45:20:42 - loss: 1.0803 - acc: 0.734 - ETA: 45:19:55 - loss: 1.0673 - acc: 0.737 - ETA: 45:13:48 - loss: 1.0547 - acc: 0.740 - ETA: 45:05:39 - loss: 1.0424 - acc: 0.743 - ETA: 45:03:41 - loss: 1.0305 - acc: 0.745 - ETA: 45:07:58 - loss: 1.0189 - acc: 0.748 - ETA: 45:03:48 - loss: 1.0076 - acc: 0.750 - ETA: 44:57:55 - loss: 0.9966 - acc: 0.753 - ETA: 45:04:05 - loss: 0.9859 - acc: 0.755 - ETA: 45:03:20 - loss: 0.9755 - acc: 0.758 - ETA: 44:54:04 - loss: 0.9653 - acc: 0.760 - ETA: 44:54:43 - loss: 0.9554 - acc: 0.762 - ETA: 44:52:40 - loss: 0.9458 - acc: 0.764 - ETA: 44:44:49 - loss: 0.9363 - acc: 0.766 - ETA: 44:43:24 - loss: 0.9271 - acc: 0.769 - ETA: 44:42:12 - loss: 0.9181 - acc: 0.771 - ETA: 44:34:11 - loss: 0.9094 - acc: 0.773 - ETA: 44:33:23 - loss: 0.9008 - acc: 0.774 - ETA: 44:30:04 - loss: 0.8924 - acc: 0.776 - ETA: 44:23:00 - loss: 0.8842 - acc: 0.778 - ETA: 44:21:42 - loss: 0.8761 - acc: 0.780 - ETA: 44:10:09 - loss: 0.8683 - acc: 0.782 - ETA: 44:01:48 - loss: 0.8606 - acc: 0.784 - ETA: 44:01:19 - loss: 0.8531 - acc: 0.785 - ETA: 43:59:51 - loss: 0.8457 - acc: 0.787 - ETA: 43:53:01 - loss: 0.8385 - acc: 0.789 - ETA: 43:52:21 - loss: 0.8314 - acc: 0.790 - ETA: 43:51:45 - loss: 0.8244 - acc: 0.792 - ETA: 43:46:41 - loss: 0.8176 - acc: 0.794 - ETA: 43:47:01 - loss: 0.8109 - acc: 0.795 - ETA: 43:45:23 - loss: 0.8043 - acc: 0.797 - ETA: 43:39:54 - loss: 0.7979 - acc: 0.798 - ETA: 43:38:35 - loss: 0.7916 - acc: 0.800 - ETA: 43:36:23 - loss: 0.7854 - acc: 0.801 - ETA: 43:32:39 - loss: 0.7792 - acc: 0.803 - ETA: 43:24:35 - loss: 0.7732 - acc: 0.804 - ETA: 43:25:13 - loss: 0.7673 - acc: 0.805 - ETA: 43:25:49 - loss: 0.7616 - acc: 0.807 - ETA: 43:18:45 - loss: 0.7559 - acc: 0.808 - ETA: 43:18:51 - loss: 0.7502 - acc: 0.809 - ETA: 43:15:53 - loss: 0.7447 - acc: 0.811 - ETA: 43:10:16 - loss: 0.7393 - acc: 0.812 - ETA: 43:10:44 - loss: 0.7339 - acc: 0.813 - ETA: 43:12:27 - loss: 0.7287 - acc: 0.815 - ETA: 43:07:47 - loss: 0.7235 - acc: 0.816 - ETA: 42:58:42 - loss: 0.7184 - acc: 0.817 - ETA: 42:50:26 - loss: 0.7134 - acc: 0.818 - ETA: 42:44:50 - loss: 0.7084 - acc: 0.819 - ETA: 42:42:43 - loss: 0.7035 - acc: 0.821 - ETA: 42:41:01 - loss: 0.6987 - acc: 0.822 - ETA: 42:39:30 - loss: 0.6940 - acc: 0.823 - ETA: 42:36:38 - loss: 0.6893 - acc: 0.824 - ETA: 42:34:39 - loss: 0.6847 - acc: 0.825 - ETA: 42:28:49 - loss: 0.6802 - acc: 0.826 - ETA: 42:25:41 - loss: 0.6757 - acc: 0.827 - ETA: 42:28:11 - loss: 0.6713 - acc: 0.828 - ETA: 42:28:55 - loss: 0.6669 - acc: 0.829 - ETA: 42:27:17 - loss: 0.6626 - acc: 0.830 - ETA: 42:19:51 - loss: 0.6584 - acc: 0.831 - ETA: 42:15:31 - loss: 0.6542 - acc: 0.832 - ETA: 42:12:43 - loss: 0.6500 - acc: 0.833 - ETA: 42:09:56 - loss: 0.6459 - acc: 0.834 - ETA: 42:09:17 - loss: 0.6419 - acc: 0.835 - ETA: 42:07:09 - loss: 0.6379 - acc: 0.836 - ETA: 42:06:05 - loss: 0.6340 - acc: 0.837 - ETA: 42:04:32 - loss: 0.6301 - acc: 0.838 - ETA: 42:03:36 - loss: 0.6263 - acc: 0.839 - ETA: 42:01:48 - loss: 0.6225 - acc: 0.840 - ETA: 41:59:48 - loss: 0.6188 - acc: 0.841 - ETA: 41:57:43 - loss: 0.6151 - acc: 0.842 - ETA: 41:53:46 - loss: 0.6114 - acc: 0.843 - ETA: 41:53:40 - loss: 0.6078 - acc: 0.844 - ETA: 41:56:11 - loss: 0.6042 - acc: 0.845 - ETA: 41:53:15 - loss: 0.6007 - acc: 0.845 - ETA: 41:52:41 - loss: 0.5972 - acc: 0.846 - ETA: 41:54:15 - loss: 0.5938 - acc: 0.847 - ETA: 41:50:12 - loss: 0.5904 - acc: 0.848 - ETA: 41:52:38 - loss: 0.5870 - acc: 0.849 - ETA: 41:54:17 - loss: 0.5837 - acc: 0.850 - ETA: 41:49:38 - loss: 0.5804 - acc: 0.850 - ETA: 41:50:20 - loss: 0.5772 - acc: 0.851 - ETA: 41:50:33 - loss: 0.5739 - acc: 0.852 - ETA: 41:48:43 - loss: 0.5708 - acc: 0.853 - ETA: 41:49:47 - loss: 0.5676 - acc: 0.854 - ETA: 41:46:39 - loss: 0.5645 - acc: 0.854 - ETA: 41:49:53 - loss: 0.5614 - acc: 0.855 - ETA: 41:49:32 - loss: 0.5584 - acc: 0.856 - ETA: 41:46:36 - loss: 0.5554 - acc: 0.857 - ETA: 41:47:14 - loss: 0.5524 - acc: 0.857 - ETA: 41:46:05 - loss: 0.5495 - acc: 0.858 - ETA: 41:42:33 - loss: 0.5465 - acc: 0.859 - ETA: 41:41:59 - loss: 0.5437 - acc: 0.859 - ETA: 41:40:21 - loss: 0.5408 - acc: 0.860 - ETA: 41:39:54 - loss: 0.5380 - acc: 0.861 - ETA: 41:39:51 - loss: 0.5352 - acc: 0.862 - ETA: 41:41:05 - loss: 0.5324 - acc: 0.862 - ETA: 41:35:45 - loss: 0.5297 - acc: 0.863 - ETA: 41:31:53 - loss: 0.5270 - acc: 0.864 - ETA: 41:30:08 - loss: 0.5243 - acc: 0.864 - ETA: 41:29:56 - loss: 0.5217 - acc: 0.865 - ETA: 41:25:53 - loss: 0.5190 - acc: 0.866 - ETA: 41:26:17 - loss: 0.5164 - acc: 0.866 - ETA: 41:22:52 - loss: 0.5139 - acc: 0.867 - ETA: 41:23:09 - loss: 0.5113 - acc: 0.868 - ETA: 41:22:54 - loss: 0.5088 - acc: 0.868 - ETA: 41:20:31 - loss: 0.5063 - acc: 0.869 - ETA: 41:19:04 - loss: 0.5038 - acc: 0.869 - ETA: 41:16:25 - loss: 0.5014 - acc: 0.870 - ETA: 41:18:00 - loss: 0.4990 - acc: 0.871 - ETA: 41:16:30 - loss: 0.4966 - acc: 0.871 - ETA: 41:17:37 - loss: 0.4942 - acc: 0.872 - ETA: 41:16:11 - loss: 0.4918 - acc: 0.872 - ETA: 41:20:52 - loss: 0.4895 - acc: 0.873 - ETA: 41:22:02 - loss: 0.4872 - acc: 0.874 - ETA: 41:21:49 - loss: 0.4849 - acc: 0.874 - ETA: 41:18:22 - loss: 0.4827 - acc: 0.875 - ETA: 41:17:47 - loss: 0.4804 - acc: 0.875 - ETA: 41:18:43 - loss: 0.4782 - acc: 0.876 - ETA: 41:15:30 - loss: 0.4760 - acc: 0.876 - ETA: 41:16:07 - loss: 0.4738 - acc: 0.877 - ETA: 41:15:48 - loss: 0.4717 - acc: 0.877 - ETA: 41:14:04 - loss: 0.4695 - acc: 0.878 - ETA: 41:14:24 - loss: 0.4674 - acc: 0.878 - ETA: 41:16:02 - loss: 0.4653 - acc: 0.879 - ETA: 41:14:18 - loss: 0.4632 - acc: 0.880 - ETA: 41:13:22 - loss: 0.4612 - acc: 0.880 - ETA: 41:09:44 - loss: 0.4591 - acc: 0.881 - ETA: 41:08:58 - loss: 0.4571 - acc: 0.881 - ETA: 41:07:40 - loss: 0.4551 - acc: 0.882 - ETA: 41:04:27 - loss: 0.4531 - acc: 0.8825\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 371/1875 [====>.........................] - ETA: 41:05:01 - loss: 0.4512 - acc: 0.883 - ETA: 41:00:20 - loss: 0.4492 - acc: 0.883 - ETA: 40:57:36 - loss: 0.4473 - acc: 0.884 - ETA: 40:57:35 - loss: 0.4454 - acc: 0.884 - ETA: 40:56:47 - loss: 0.4435 - acc: 0.885 - ETA: 40:54:05 - loss: 0.4416 - acc: 0.885 - ETA: 40:53:46 - loss: 0.4397 - acc: 0.885 - ETA: 40:53:40 - loss: 0.4379 - acc: 0.886 - ETA: 40:53:17 - loss: 0.4361 - acc: 0.886 - ETA: 40:51:20 - loss: 0.4342 - acc: 0.887 - ETA: 40:51:36 - loss: 0.4324 - acc: 0.887 - ETA: 40:48:58 - loss: 0.4307 - acc: 0.888 - ETA: 40:49:48 - loss: 0.4289 - acc: 0.888 - ETA: 40:46:33 - loss: 0.4271 - acc: 0.889 - ETA: 40:45:17 - loss: 0.4254 - acc: 0.889 - ETA: 40:44:53 - loss: 0.4237 - acc: 0.890 - ETA: 40:42:20 - loss: 0.4220 - acc: 0.890 - ETA: 40:42:43 - loss: 0.4203 - acc: 0.890 - ETA: 40:40:52 - loss: 0.4186 - acc: 0.891 - ETA: 40:39:14 - loss: 0.4169 - acc: 0.891 - ETA: 40:39:50 - loss: 0.4152 - acc: 0.892 - ETA: 43:09:36 - loss: 0.4136 - acc: 0.892 - ETA: 43:09:47 - loss: 0.4120 - acc: 0.893 - ETA: 43:09:23 - loss: 0.4104 - acc: 0.893 - ETA: 43:06:03 - loss: 0.4088 - acc: 0.893 - ETA: 43:06:04 - loss: 0.4072 - acc: 0.894 - ETA: 43:05:01 - loss: 0.4056 - acc: 0.894 - ETA: 43:01:27 - loss: 0.4040 - acc: 0.895 - ETA: 43:00:47 - loss: 0.4025 - acc: 0.895 - ETA: 42:59:40 - loss: 0.4010 - acc: 0.895 - ETA: 42:58:23 - loss: 0.3994 - acc: 0.896 - ETA: 42:59:26 - loss: 0.3979 - acc: 0.896 - ETA: 42:59:57 - loss: 0.3964 - acc: 0.896 - ETA: 42:59:08 - loss: 0.3949 - acc: 0.897 - ETA: 43:01:40 - loss: 0.3934 - acc: 0.897 - ETA: 43:03:20 - loss: 0.3920 - acc: 0.898 - ETA: 43:00:58 - loss: 0.3905 - acc: 0.898 - ETA: 43:02:16 - loss: 0.3891 - acc: 0.898 - ETA: 43:03:24 - loss: 0.3876 - acc: 0.899 - ETA: 43:02:43 - loss: 0.3862 - acc: 0.899 - ETA: 43:04:41 - loss: 0.3848 - acc: 0.899 - ETA: 43:04:36 - loss: 0.3834 - acc: 0.900 - ETA: 43:02:40 - loss: 0.3820 - acc: 0.900 - ETA: 43:03:55 - loss: 0.3806 - acc: 0.900 - ETA: 43:06:32 - loss: 0.3793 - acc: 0.901 - ETA: 43:05:57 - loss: 0.3779 - acc: 0.901 - ETA: 43:01:37 - loss: 0.3766 - acc: 0.902 - ETA: 42:59:40 - loss: 0.3752 - acc: 0.902 - ETA: 42:56:37 - loss: 0.3739 - acc: 0.902 - ETA: 42:55:26 - loss: 0.3726 - acc: 0.903 - ETA: 42:53:58 - loss: 0.3713 - acc: 0.903 - ETA: 42:53:51 - loss: 0.3700 - acc: 0.903 - ETA: 42:52:34 - loss: 0.3687 - acc: 0.904 - ETA: 42:49:39 - loss: 0.3674 - acc: 0.904 - ETA: 42:50:18 - loss: 0.3662 - acc: 0.904 - ETA: 42:47:46 - loss: 0.3649 - acc: 0.905 - ETA: 42:46:30 - loss: 0.3636 - acc: 0.905 - ETA: 42:44:18 - loss: 0.3624 - acc: 0.905 - ETA: 42:44:23 - loss: 0.3612 - acc: 0.905 - ETA: 42:41:31 - loss: 0.3599 - acc: 0.906 - ETA: 42:40:34 - loss: 0.3587 - acc: 0.906 - ETA: 42:38:08 - loss: 0.3575 - acc: 0.906 - ETA: 42:37:18 - loss: 0.3563 - acc: 0.907 - ETA: 42:35:31 - loss: 0.3551 - acc: 0.907 - ETA: 42:32:27 - loss: 0.3539 - acc: 0.907 - ETA: 42:31:11 - loss: 0.3528 - acc: 0.908 - ETA: 42:27:40 - loss: 0.3516 - acc: 0.908 - ETA: 42:26:05 - loss: 0.3504 - acc: 0.908 - ETA: 42:23:15 - loss: 0.3493 - acc: 0.908 - ETA: 42:21:57 - loss: 0.3481 - acc: 0.909 - ETA: 42:20:06 - loss: 0.3470 - acc: 0.909 - ETA: 42:16:40 - loss: 0.3459 - acc: 0.909 - ETA: 42:15:29 - loss: 0.3448 - acc: 0.910 - ETA: 42:12:47 - loss: 0.3436 - acc: 0.910 - ETA: 42:09:23 - loss: 0.3425 - acc: 0.910 - ETA: 42:09:16 - loss: 0.3414 - acc: 0.910 - ETA: 42:07:16 - loss: 0.3404 - acc: 0.911 - ETA: 42:06:20 - loss: 0.3393 - acc: 0.911 - ETA: 42:03:18 - loss: 0.3382 - acc: 0.911 - ETA: 42:03:18 - loss: 0.3371 - acc: 0.912 - ETA: 42:02:12 - loss: 0.3361 - acc: 0.912 - ETA: 41:59:10 - loss: 0.3350 - acc: 0.912 - ETA: 41:59:19 - loss: 0.3340 - acc: 0.912 - ETA: 41:57:00 - loss: 0.3329 - acc: 0.913 - ETA: 41:57:18 - loss: 0.3319 - acc: 0.913 - ETA: 41:57:48 - loss: 0.3309 - acc: 0.913 - ETA: 41:55:28 - loss: 0.3298 - acc: 0.913 - ETA: 41:55:37 - loss: 0.3288 - acc: 0.914 - ETA: 41:55:40 - loss: 0.3278 - acc: 0.914 - ETA: 41:55:31 - loss: 0.3268 - acc: 0.914 - ETA: 41:55:05 - loss: 0.3258 - acc: 0.914 - ETA: 41:51:17 - loss: 0.3248 - acc: 0.915 - ETA: 41:48:31 - loss: 0.3239 - acc: 0.915 - ETA: 41:44:30 - loss: 0.3229 - acc: 0.915 - ETA: 41:41:24 - loss: 0.3219 - acc: 0.915 - ETA: 41:40:27 - loss: 0.3209 - acc: 0.916 - ETA: 41:37:53 - loss: 0.3200 - acc: 0.916 - ETA: 41:34:32 - loss: 0.3190 - acc: 0.916 - ETA: 41:32:30 - loss: 0.3181 - acc: 0.916 - ETA: 41:32:18 - loss: 0.3171 - acc: 0.917 - ETA: 41:29:49 - loss: 0.3162 - acc: 0.917 - ETA: 41:28:54 - loss: 0.3153 - acc: 0.917 - ETA: 41:28:05 - loss: 0.3144 - acc: 0.917 - ETA: 41:25:07 - loss: 0.3134 - acc: 0.918 - ETA: 41:23:39 - loss: 0.3125 - acc: 0.918 - ETA: 41:20:48 - loss: 0.3116 - acc: 0.918 - ETA: 41:19:40 - loss: 0.3107 - acc: 0.918 - ETA: 41:15:21 - loss: 0.3098 - acc: 0.919 - ETA: 41:12:21 - loss: 0.3089 - acc: 0.919 - ETA: 41:10:53 - loss: 0.3081 - acc: 0.919 - ETA: 41:09:42 - loss: 0.3072 - acc: 0.919 - ETA: 41:08:35 - loss: 0.3063 - acc: 0.919 - ETA: 41:05:29 - loss: 0.3054 - acc: 0.920 - ETA: 41:02:30 - loss: 0.3046 - acc: 0.920 - ETA: 41:00:32 - loss: 0.3037 - acc: 0.920 - ETA: 40:57:53 - loss: 0.3029 - acc: 0.920 - ETA: 40:55:49 - loss: 0.3020 - acc: 0.921 - ETA: 40:52:11 - loss: 0.3012 - acc: 0.921 - ETA: 40:49:57 - loss: 0.3003 - acc: 0.921 - ETA: 40:47:37 - loss: 0.2995 - acc: 0.921 - ETA: 40:45:43 - loss: 0.2987 - acc: 0.921 - ETA: 40:43:32 - loss: 0.2979 - acc: 0.922 - ETA: 40:41:20 - loss: 0.2970 - acc: 0.922 - ETA: 40:39:18 - loss: 0.2962 - acc: 0.922 - ETA: 40:36:37 - loss: 0.2954 - acc: 0.922 - ETA: 40:34:58 - loss: 0.2946 - acc: 0.922 - ETA: 41:07:59 - loss: 0.2938 - acc: 0.923 - ETA: 41:05:55 - loss: 0.2930 - acc: 0.923 - ETA: 41:02:56 - loss: 0.2922 - acc: 0.923 - ETA: 41:01:40 - loss: 0.2914 - acc: 0.923 - ETA: 41:00:44 - loss: 0.2907 - acc: 0.924 - ETA: 40:59:33 - loss: 0.2899 - acc: 0.924 - ETA: 40:58:24 - loss: 0.2891 - acc: 0.924 - ETA: 40:55:48 - loss: 0.2883 - acc: 0.924 - ETA: 40:55:12 - loss: 0.2876 - acc: 0.924 - ETA: 40:54:21 - loss: 0.2868 - acc: 0.925 - ETA: 40:53:40 - loss: 0.2861 - acc: 0.925 - ETA: 40:50:54 - loss: 0.2853 - acc: 0.925 - ETA: 40:50:20 - loss: 0.2846 - acc: 0.925 - ETA: 40:49:43 - loss: 0.2838 - acc: 0.925 - ETA: 40:47:54 - loss: 0.2831 - acc: 0.925 - ETA: 40:45:20 - loss: 0.2823 - acc: 0.926 - ETA: 40:44:38 - loss: 0.2816 - acc: 0.926 - ETA: 40:42:41 - loss: 0.2809 - acc: 0.926 - ETA: 40:39:58 - loss: 0.2802 - acc: 0.926 - ETA: 40:38:24 - loss: 0.2794 - acc: 0.926 - ETA: 40:37:01 - loss: 0.2787 - acc: 0.927 - ETA: 40:35:47 - loss: 0.2780 - acc: 0.927 - ETA: 40:34:13 - loss: 0.2773 - acc: 0.927 - ETA: 40:30:12 - loss: 0.2766 - acc: 0.927 - ETA: 40:28:01 - loss: 0.2759 - acc: 0.927 - ETA: 40:25:22 - loss: 0.2752 - acc: 0.927 - ETA: 40:23:24 - loss: 0.2745 - acc: 0.928 - ETA: 40:22:35 - loss: 0.2738 - acc: 0.928 - ETA: 40:20:26 - loss: 0.2731 - acc: 0.928 - ETA: 40:17:35 - loss: 0.2724 - acc: 0.928 - ETA: 40:17:23 - loss: 0.2718 - acc: 0.928 - ETA: 40:14:53 - loss: 0.2711 - acc: 0.929 - ETA: 40:12:48 - loss: 0.2704 - acc: 0.929 - ETA: 40:11:02 - loss: 0.2697 - acc: 0.929 - ETA: 40:09:27 - loss: 0.2691 - acc: 0.929 - ETA: 40:07:38 - loss: 0.2684 - acc: 0.929 - ETA: 40:07:32 - loss: 0.2678 - acc: 0.929 - ETA: 40:06:29 - loss: 0.2671 - acc: 0.930 - ETA: 40:04:54 - loss: 0.2665 - acc: 0.930 - ETA: 40:02:26 - loss: 0.2658 - acc: 0.930 - ETA: 39:59:49 - loss: 0.2652 - acc: 0.930 - ETA: 39:59:13 - loss: 0.2645 - acc: 0.930 - ETA: 39:57:13 - loss: 0.2639 - acc: 0.930 - ETA: 39:56:15 - loss: 0.2632 - acc: 0.931 - ETA: 39:54:45 - loss: 0.2626 - acc: 0.931 - ETA: 39:52:10 - loss: 0.2620 - acc: 0.931 - ETA: 39:50:45 - loss: 0.2614 - acc: 0.931 - ETA: 39:49:01 - loss: 0.2607 - acc: 0.931 - ETA: 39:46:37 - loss: 0.2601 - acc: 0.931 - ETA: 39:44:31 - loss: 0.2595 - acc: 0.932 - ETA: 39:42:43 - loss: 0.2589 - acc: 0.932 - ETA: 39:40:24 - loss: 0.2583 - acc: 0.932 - ETA: 39:38:01 - loss: 0.2577 - acc: 0.932 - ETA: 39:36:45 - loss: 0.2571 - acc: 0.932 - ETA: 39:34:46 - loss: 0.2565 - acc: 0.932 - ETA: 39:32:08 - loss: 0.2559 - acc: 0.932 - ETA: 39:30:27 - loss: 0.2553 - acc: 0.933 - ETA: 39:29:24 - loss: 0.2547 - acc: 0.933 - ETA: 39:28:01 - loss: 0.2541 - acc: 0.933 - ETA: 39:26:57 - loss: 0.2535 - acc: 0.9335"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 407/1875 [=====>........................] - ETA: 39:25:49 - loss: 0.2529 - acc: 0.933 - ETA: 39:22:51 - loss: 0.2523 - acc: 0.933 - ETA: 39:21:12 - loss: 0.2518 - acc: 0.934 - ETA: 39:19:36 - loss: 0.2512 - acc: 0.934 - ETA: 39:17:40 - loss: 0.2506 - acc: 0.934 - ETA: 39:15:08 - loss: 0.2501 - acc: 0.934 - ETA: 39:14:41 - loss: 0.2495 - acc: 0.934 - ETA: 39:13:09 - loss: 0.2489 - acc: 0.934 - ETA: 39:10:37 - loss: 0.2484 - acc: 0.934 - ETA: 39:09:22 - loss: 0.2478 - acc: 0.935 - ETA: 39:07:49 - loss: 0.2472 - acc: 0.935 - ETA: 39:05:26 - loss: 0.2467 - acc: 0.935 - ETA: 39:04:17 - loss: 0.2461 - acc: 0.935 - ETA: 39:02:05 - loss: 0.2456 - acc: 0.935 - ETA: 38:59:17 - loss: 0.2450 - acc: 0.935 - ETA: 92:40:59 - loss: 0.2445 - acc: 0.935 - ETA: 92:30:29 - loss: 0.2440 - acc: 0.936 - ETA: 92:19:49 - loss: 0.2434 - acc: 0.936 - ETA: 92:09:27 - loss: 0.2429 - acc: 0.936 - ETA: 91:58:29 - loss: 0.2423 - acc: 0.936 - ETA: 91:45:41 - loss: 0.2418 - acc: 0.936 - ETA: 91:34:26 - loss: 0.2413 - acc: 0.936 - ETA: 91:25:12 - loss: 0.2408 - acc: 0.936 - ETA: 91:13:48 - loss: 0.2402 - acc: 0.937 - ETA: 91:04:53 - loss: 0.2397 - acc: 0.937 - ETA: 90:54:12 - loss: 0.2392 - acc: 0.937 - ETA: 90:42:11 - loss: 0.2387 - acc: 0.937 - ETA: 90:30:21 - loss: 0.2382 - acc: 0.937 - ETA: 90:20:26 - loss: 0.2376 - acc: 0.937 - ETA: 90:10:03 - loss: 0.2371 - acc: 0.937 - ETA: 89:58:36 - loss: 0.2366 - acc: 0.937 - ETA: 89:49:35 - loss: 0.2361 - acc: 0.938 - ETA: 89:38:25 - loss: 0.2356 - acc: 0.938 - ETA: 89:29:50 - loss: 0.2351 - acc: 0.938 - ETA: 89:20:12 - loss: 0.2346 - acc: 0.938 - ETA: 89:09:53 - loss: 0.2341 - acc: 0.9386"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "#***mistake I had done was... I did'nt include y_train,.. and I wasted almost an hour to debug this...!!!\n",
    "\n",
    "model.fit(x_train, y_train, epochs = 5, steps_per_epoch=num_train_examples//BATCH_SIZE, shuffle=True, \n",
    "          batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
