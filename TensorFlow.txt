 "if you try to grab it, it runs away... if you are centered and happy it comes to you.." Sri Sri

Writing convnet in oops way....
This is my notes about how to create a convnet using tensor flow, without using APIs.

Challenges..
know the architecture..
how exactly tensorflow works..?
`use tf.layers... version 1.10`
<https://medium.com/google-developer-experts/demystify-the-tensorflow-apis-57d2b0b8b6c0>
I need to know both things....
low level API tf.nn, which is really time consuming, but gives, total control..
high level API, tf.layer and tf.keras... 
Way to go man....!

Tensorflow model bilding APIs
low level: tf
middle level: tf.layer
high level: tf.keras
...
<https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/?source=post_page--------------------------->
Most python libraries comes with natural extension, like variables, functions, classes etc... but its not like that for tensirflow. Tensorflow is framework for representing certain type of computational abstraction.
first thing is to assemble computational graph, second is interact with it using tensorflow session

COMPUTATION GRAPH:
Its a global data structure
`import tensorflow as tf
two_node = tf.constant(2)
print(two_node)`
when this is executed
`Tensor("Const:0", shape=(), dtype=int32)`
We got ourselves a node. It contains the constant 2. This is a pointer

its dataflow graph, like with edges and nodes.., here nodes are like operations, possibly applied to input data, and generate another data which is passed to next node.
like this nodes have dependencies on other nodes. tf actually localises these dependencies and execute only required nodes, and hence reduce the execution time...(src, Learning tensorflow pdf)..
Example...
`a = tf.constant(5)
b = tf.constant(2)
c = tf.constant(3)
d = tf.multiply(a,b)
e = tf.add(c,b)
f = tf.subtract(d,e)`

this is creating computational graph..once this is done, we need to run this.

To run this we need to create and run a session 
`sess = tf.Session()
outs = sess.run(f)
sess.close()

print("outs = {}".format(outs))`

it starts at the requested output(s) and then works backward,
computing nodes that must be executed according to the set of dependencies.

Variables..
it has two stages
1. declare variable
2. initialize by running session using global_variables_initializer

Placeholder...
these are like empty variables to be filled with data later on.



.
.
.

Cross entropy is a measure of similarity between two distributions.

This is where I was confused.. now got it.. tensorflow does the following three lines inside `sigmoid_cross_entropy_with_logits`

`y_pred = tf.sigmoid(y_pred)
loss = y_true*tf.log(y_pred) - (1-y_true)*tf.log(1-y_pred)
loss = tf.reduce_mean(loss)`

Luckily, TensorFlow already has a designated function we can use instead:

`tf.nn.sigmoid_cross_entropy_with_logits(labels=,logits=)`

Logits simply means unscaled output of earlier layers.
Labels are proper train y labels(one hot encoded)

`mean(y == y_pred)` is what accuracy is.. i,e how many right guesses.. this done by

`correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))`

y_pred = np.array([[31, 23,  4, 24, 27, 34],
                [18,  3, 25,  0,  6, 35],
                [28, 14, 33, 22, 20,  8],
                [13, 30, 21, 19,  7,  9],
                [16,  1, 26, 32,  2, 29],
                [17, 12,  5, 11, 10, 15]])
                
Evaluating `tf.argmax(y_pred, 1)` gives a tensor whose evaluation will give `array([5, 5, 2, 1, 3, 0])`

y = np.array([[31, 23,  4, 24, 27, 34],
                [18,  3, 25,  0,  6, 35],
                [28, 14, 33, 22, 20,  8],
                [13, 30, 21, 19,  7,  9],
                [16,  1, 26, 32,  2, 29],
                [17, 12,  5, 11, 10, 15]])
                
Evaluating `tf.argmax(y, 1)` gives a tensor whose evaluation will give `array([5, 5, 2, 1, 3, 0])`

`tf.equal(tf.argmax(y_pred, 1),tf.argmax(y, 1))` returns a tensor whose evaluation will give `array(1,1,1,1,1,1)`


a very good explanation
<https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits>